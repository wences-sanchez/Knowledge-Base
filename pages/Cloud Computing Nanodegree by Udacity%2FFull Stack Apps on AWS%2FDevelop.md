public:: true
tags:: Udacity, Cloud Development
deck:: [[Cloud Development Nanodegree::Full Stack Apps on AWS]]

-
- ## 1. Intro
- ## 2. How to design an API
	- ### What is a Server? #flashcard
	  id:: 63de231a-52c2-4c34-a004-2f33ab919356
		- A **Server** is a computer connected to a network
		- A **Cloud Server** is a computer connected to a network that is managed by someone else.
		- We only need a keyboard and software to build a server, these days.
		- We'll be using NodeJS
			- NodeJS is a JavaScript based server-side environment to allow developers to build network applications.
			-
	- ### What is an Application Program Interface (API)? #flashcard
	  id:: 63de231a-dacb-4a4a-8139-1958676718fd
		- **RESTful APIs** use request paths, types and bodies to perform a specific action
		- ![image.png](../assets/image_1666345146402_0.png)
		- ![image.png](../assets/image_1675244402173_0.png)
		- ### Other Best Practices
			- Only use nouns and no verbs. They should be plural and consistent
			- APIs should be versioned:
				- **{{host}}/api/v0/cars/5**
			- All responses should include data format (i.e. application/json)
			-
- ## 3. Standing up a Cloud Capable Server Locally using Node #flashcard
  id:: 63de231a-d8fb-43d1-aa1c-43e45f5bc6ef
	- ### Standing up a Cloud Capable Server
		- #### Introduction to Node
			- [Node (aka NodeJs)](https://nodejs.org/en/) is a powerful framework to build network applications using JavaScript (in our case using TypeScript) outside of browsers. It has an asynchronous concurrent model which releases the developer from many concerns involving threading and dead-locking. Node is used as our server framework along with [Express](https://expressjs.com/) to handle web http requests and responses.
		- **NodeJS** is a framework. And **Typescript** is the language that NodeJS uses.
		- #### Introduction to TypeScript
			- [Typescript](https://github.com/Microsoft/TypeScript) is a flavor of JavaScript that forces hard typing on variables and methods. This prevents implementation errors like passing a string instead of a number. It compiles to pure JavaScript.
		- #### Github link to basic server
			- In this lesson, we'll be referencing a prewritten, simple server which uses Node/Express. The github link to clone and follow along is: [https://github.com/udacity/cloud-developer/tree/master/course-02/exercises/udacity-c2-basic-server](https://github.com/udacity/cloud-developer/tree/master/course-02/exercises/udacity-c2-basic-server). Alternatively, you can clone the repo for the entire nanodegree [cloud-developer](https://github.com/udacity/cloud-developer) and find the files for this course in the path [https://github.com/udacity/cloud-developer/tree/master/course-02/exercises](https://github.com/udacity/cloud-developer/tree/master/course-02/exercises)
		- #### Installing project dependencies
			- This project uses NPM to manage software dependencies. NPM Relies on the package.json file located in the root of this repository. After cloning, open your terminal in the repo directory and run:
			- ```
			  npm install
			  ```
			  
			  > *tip*: **npm i** is shorthand for **npm install**
		- #### Installing Postman
		- We'll be using a tool called Postman to make requests to our development server. Download and install [Postman](https://www.getpostman.com/downloads/)
	- ### Clone Our Server Repository
		- {{video https://www.youtube.com/watch?v=Lb5xjDfnnBA&t=1s}}
		  collapsed:: true
			- {{youtube-timestamp 0}} so we're going to start with this coding exercise by cloning our basic server repo we'll copy our server link open our terminal CD into our development folder and get clone that repo will then CD into that repo and type code to open that up in Visual Studio code now that we have our project open let's take a quick look at the layout and structure of our server now there's a lot here and you should take a little bit of time to poke around and understand the basics of node we have a lot implemented to get you started quickly but these concepts are important and you should really become a little bit comfortable with how things are structured let's start with a simple package JSON file now you also hear me say Jason and JSON you could google the debate there's a lot going on with how do you actually say JSON I say JSON most people say Jason it's the whole thing the package JSON file contains a lot of structure for our system this is a part
			- {{youtube-timestamp 62}} of the node package manager to manage the software itself and its dependencies it has a name for the project some scripts that we'll be using throughout development some information about our git repository and then a whole bunch of software dependencies that we'll be using throughout the project we need to install these dependencies before we can do anything so let's once again open our terminal and type NPM I to install those dependencies this is fetching those packages from the NPM system and installing them into a local node modules folder now that we have our dependencies we can start to build we're using typescript and we're installing the typescript dependencies specific types for packages as well as some tools to help us in our development stage I'm not going to go into detail but the TS config and the TS lint files help the typescript system to know how to transpile our typescript into JavaScript you can ignore these files for now as
			- {{youtube-timestamp 127}} well they will be used later on in the course for other lessons
	- ### Understanding Our Server Code
		- {{video https://www.youtube.com/watch?v=Myus7ZDTG7Y&t=1s}}
		  collapsed:: true
			- {{youtube-timestamp 0}} our source directory we get into the details in the meat of our system server tes is our main code base I'm going to switch to Zen mode to make this a little bit easier for everyone to see let's break down the basic structure of this server file at the top of our page we have our imports this is using Express which is our framework for helping us to build our web server something called body parser which will help us to parse inbound requests to remove the body from that request and make use of it and then some cars from our cars file which we'll use as a pseudo data store going into our main body we can see a few things first we are creating our Express application and declaring a port that we will be listening to 80-82 is the network port that we'll be using when accessing this server resource node uses a middleware design pattern which allows us to inject specific functions into the stream of the request process in this case we're just telling it to use the
			- {{youtube-timestamp 66}} body parser for types JSON finally we get to our actual requests we'll see this design pattern repeat as we begin to build our server we have our application which is intercepting a get request with an end point of forward slash which is just route it then uses a function to understand how to handle that request this function takes two inputs one of type request and one of type response we can then process that request object which at this simple end point we are not doing and use the response object to return a status of 200 and send a message to the client
	- ### Running Our Server and Making Requests
		- Para usar una parte de la URL como parámetro de path en NodeJS, usamos: #flashcard
		  id:: 63566061-d8fd-47ca-a4fa-63790eb870bc
			- ```TypeScript
			  my_app.get ( "/persons/:name", 
			              (request: Request, response: Response) => {
			      let { name } = request.params;
			  	return response.status(200).send(`Hi, ${name}!`);
			  } );
			  ```
			- Example of call: `GET /persons/Wences` ==> `Hi, Wences!`
		- Para usar queries dentro de la URL en NodeJS, usamos: #flashcard
		  id:: 63566061-fdd8-434b-95eb-148b4e1ef6cf
			- ```TypeScript
			  my_app.get ( "/persons/",
			             (request: Request, response: Response) => {
			    	let { name } = request.query;
			    	return response.send(200).send(`Hi, ${name}!`)
			  } );
			  ```
			- Example of call: `GET /persons/?name=Wences` ==> `Hi, Wences!`
		- #### Update
			- Para pasarle los datos a un servidor, se le pueden pasar de varias maneras:
				- Como parámetro:
					- Con `URL/parametro/:valor`
				- Como request en la url:
					- Con `URL/parametro?variable=valor`
		- {{video https://www.youtube.com/watch?v=kCv19vb0k_M&t=1s}}
		  collapsed:: true
			- {{youtube-timestamp 0}} so next we're going to actually try experimenting with this endpoint we're gonna open up the terminal and we're going to type NPM Run dev which will start our server if all is well we'll see our server is running on localhost 8080 - let's highlight this and copy it because this will come in handy in just a second we can now start to interact with this server let's open postman postman is an extremely powerful tool for interfacing and working with cloud applications it allows us to issue many types of requests get post patch put delete are just a few so we can enter into postman that we want to make a get request to our root URL by clicking send we load the request and we get our expected response of welcome to the cloud we can also see some interesting data that helps us to debug and understand the request we have a status of 200 we have the time it takes for that request to happen and then the size of the result pasting that same URL into Chrome loads
			- {{youtube-timestamp 66}} welcome to the cloud but postman is useful because it allows us to save specific types of requests and also send more complicated requests that include things like parameters authorization headers bodies and also as we'll see later in this course tests to make sure that our endpoints are working correctly let's go back to the code and see how more complicated endpoints work let's say we want to personalize that greeting for a specific person we can add an endpoint which also accepts get requests that asks for a person and the person's name in the URL this is called a path parameter and we can access this information by simply disrupting the parameters variable in our requests input this is some slightly more advanced JavaScript but essentially what this is saying is unpack the variables within this variable into a variable called name now since the name is required in this case we want to do some additional checking to make sure that
			- {{youtube-timestamp 132}} everything is good before continuing the code we add a simple if statement if we don't have a name we'll send a status code of 400 some kind of malformed input and we'll send a message saying the name is required if that check passes we can then return a status code of 200 and send the message welcome to the cloud and insert the person's name let's take a quick look how this looks with our requests we'll start by adding our persons with a name that works in my case we'll say Gabe our response is as expected welcome to the cloud with my name appended to the end and a status of 200 now if I forgot to put the name what happens great our status is 400 and we receive a message saying the name is required there are other ways to pass data to our server the next example uses query parameters or the data that's passed after a question mark in the URL in this case we're passing a name parameter and then the value of that parameter just like we had before we can
			- {{youtube-timestamp 199}} use destruction to unpack the value within the query property of the request input we once again can check to make sure we have a name and if we do make use of it and return our 200 status code in postman we can add that same format name is equal to Chris click send and we see our result is working one thing that postman does for us is also gives us a nice GUI to interact with these kinds of variables here we can change the name to Jessica click send and we get a response once again this field is required so for yet leave it empty we expect to retrieve 400 with a name is required response
	- ### More Complicated POST Requests
		- {{video https://www.youtube.com/watch?v=EEb1EFVQxms}}
		  collapsed:: true
			- {{youtube-timestamp 0}} finally we can also post the data to the cloud posting is a little bit more complicated and requires a bit more overhead in our request but it has the same general structure within our node environment we simply unpack request dot body which is accessible because we're using the body parser middleware that we defined earlier after passing the same checks we return the same result let's see how to do this in postman we can set our endpoint to persons change this from get to post and now we need to do a few extra things we need to add a body to our request we do this by clicking on the body tab clicking raw and entering in some JSON by clicking send we retrieve the response from our server as expected we insert the name with the status of two hundred one thing that this did was also added our data type in this case application JSON to our headers so that the server knew what kind of body payload was included if we did not include that or had a malformed
			- {{youtube-timestamp 79}} a malformed header that did not match our data type well we're not going to have that payload and retrieve a 400 bad request although this probably should have failed slightly differently um one thing I did not mention scrolling to the bottom and by passing some of that code we see app dot listen this starts the server and tells it to listen on the port that we defined earlier now that we've covered the basics of how to define endpoints why don't you try it on your own within this basic server we've included a list of cars this is a good opportunity to understand why typescript is so powerful going up to the top we're importing some variable called cars and this is where we're going to use it we define a local instance of this cars variable which is going to be of type car array which basically just means a list of cars control clicking on this will open up that file here we define the car interface which gives us a make type model cost
			- {{youtube-timestamp 137}} and ID and the type of data that they include and then we give a little stubbed set of cars that we will be interacting with we can use this variable within our new endpoints that we'll be experimenting with to provide a more dynamic data experience for our users go ahead and try adding an endpoint to get the entire list of cars try adding an endpoint to get a specific car by its ID and try adding an endpoint to add a new car to that list now keep in mind since we're working with an instance variable within this server that variable will reset to its default state whenever it resets and since we're using some fun tooling to use typescript it resets every time we save the file so don't be surprised if after you post it won't necessarily show your data if you've made a change
	- ### Code Exercise
		- Read the comments in the `server.ts` file that prompt you to create some requests on your own. You can follow along in the solution video by viewing the solutions in `server.solution.ts`
	- ### Code walkthrough of Solution
		- After writing the code, save the file `server.ts`. Then start the server by going to the command line, within the directory `udacity-c2-basic-server`, and typing `npm run dev`. When you get to testing the endpoints using Postman, the server needs to be running in order for the requests to have a response.
		- {{video https://www.youtube.com/watch?v=rRSFvfHJT6A}}
		  collapsed:: true
			- {{youtube-timestamp 0}} for your coding exercise we asked you to complete some endpoints to interact with a list of cars let's see one possible solution for this exercise one endpoint we asked for was to provide the entire list of cars on request it should also be filterable with a query parameter so we add apt-get we tell it we're looking at the endpoint for cars and then we're pulling out the make variable from our query parameter we then take our cars list if we have a make variable we will use the filter method on that list to search for cars that match this criteria function what this function does is for every car object within that list we apply this boolean filter if the make is equal to the make we're looking for in the query parameter return true if it returns false it will not be included in the returned car list finally we return that final list of cars with the status of 200 the next end point we asked for was to get a specific car by ID it
			- {{youtube-timestamp 71}} should require an ID and it should fail gracefully if no matching car is found here we can add a path parameter to our endpoint unpack that parameter as an ID if there is no ID we can return a status of 400 and send a string that the ID is required finally if there is an ID we can search for cars within our list that match that ID we can do a check to see if there is a car that has that matching ID if there is not we return 404 that the car is not found finally after all these check pests it means that our car exists and we return that object to our client the last end point we asked you to complete was to post a new car to our list again this is in our instance variable of our list and will reset every time the server resets we first unpack a lot more information from our body here we have all of the information needed to complete an object for our car if we don't have any of those fields we will return a status of 400 we then
			- {{youtube-timestamp 150}} instantiate a new car object complete with all of our parameters and we push this to our car list now we respond with the proper status of 201 and send this car object to our client let's quickly explore these end points using postman I'm going to open a collection that we already have defined on the left now we've included this collection inside of this repo you can open it simply by clicking import navigating to the repo in your local directory and opening the file called Udacity c2 basic server postman collection dot JSON importing it then you will have this collection available to you within postmen we have these endpoints in the exercise and what this allows me to do is essentially play with these endpoints without having to type anything opening a specific endpoint within our exercise directory pre-populates all of the fields required to interface with that endpoint here we have our get request for our cars endpoint clicking send we retrieve a
			- {{youtube-timestamp 217}} list of all of our car objects we also see our status code of 200 if we filter that by make by adding the query parameter clicking on cars filter endpoint pre-populates our request with our URL with our query parameter for make is Toyota clicking send we now see we only retrieve the object that match make equal Toyota looking for a specific car in this case to allows us to immediately see that single car and finally we can post a new car we have a body pre-populated for our new Ford Focus we see the post is already selected and all we have to do is click send we have a new object returned to us and if we go back to our cars all end point and click send will now see our Ford Focus is included in that list finally we can find a specific car for 9 ID 9 is our new Ford Focus and if we try 10 will see 404
-
- ## 4.Verification Testing
	- ### Introduction
		- The following video will cover units tests and integration tests.
		- ### Unit Tests
			- Ensure our atomic functions and methods perform their tasks correctly or fail appropriately. We'll be playing with [Mocha and Chai](https://mochajs.org/) as our unit testing framework. We'll be covering the basics so checkout the docs!
		- ### Integration Tests
			- Integration Tests ensure every endpoint in our software package perform their tasks correctly, fails appropriately, and communicates with other systems in a predictable manner (so they integrate properly). We'll be playing with [Postman](https://www.getpostman.com/) as our integration testing framework. We'll be covering the basics so checkout the docs!
		- {{video https://www.youtube.com/watch?v=njB2Xgme6Aw}}
			- {{youtube-timestamp 0}} now let's try implementing some integration tests will be using postman to test the endpoints of our simple web server taking a look at our stack we have a lot of moving parts how do we think that this could possibly go wrong first each individual component could fail in its own responsibilities for example one of our endpoints could be trying to collect data from a data store but return the wrong objects or return the objects in the wrong format additionally our process image methods might ingest a file but apply the wrong mathematical function we can check against these kinds of issues by implementing something called unit tests and as our cloud begins to grow and we have more things interacting with our subsystems it becomes more important to have these kinds of testing routines implemented within our code
	- ### Defining Unit Tests with Mocha and Chai
		- {{video https://www.youtube.com/watch?v=njyFOMi0bW4}}
		  collapsed:: true
			- {{youtube-timestamp 0}} now let's try implementing some unit tests we'll be exploring two simple functions and walking through a couple simple examples of how we can cover those functions with tests will be using mocha and chai as a testing framework for art type script files opening our basic server repo within our source file we see a folder called unit tests example here we have two files units TS and unit tests TS first let's open units TS this contains two really simple unit functions that basically just abstract our math functions adding and dividing we are exporting these as constants so that we can use them within our testing framework the add function takes in two parameters a and B which are both numbers and it returns the sum of these two values the divide method also takes two numbers a and B and divides a by B let's open units tests TS here we import those two methods add and divide from our units file and we import a few helpful tools to work with our testing
			- {{youtube-timestamp 77}} framework mocha and chai mocha and chai allow us to use simple described and it methods to declare what our methods should do as well as expect methods to allow us to test whether it's actually achieving those results first let's test our add function we start by declaring the tests within this block are our add function we provide a function which then has two sub blocks each it block should be small and concise and represent a single type of thing that that function should be able to do for example it should add two and two here we consume our method ad giving it two different inputs two and two and storing that result in our result variable finally we expect our result to equal 4 we can declare a second block that declares it should add -2 and 2 again we can provide that method those inputs and check to make sure that that does indeed equal 0 the next block declares the methods for the divide function the first block simply states
			- {{youtube-timestamp 162}} it should divide 6 by 3 and we test that in the same way similarly for dividing 5 and 2 it's important to try to test multiple types of cases because things like whether or not floats will work sometimes might not be what you expect depending on the language we introduce a new type of test in this last block it should throw an error if divided by 0 obviously we can't divide by 0 so if we try to we should have a way for that method to fail gracefully we expect that that method when given the input as 0 as our second argument will throw the error divided by 0
	- ### Running Mocha and Chai Tests
		- The command `npm test` is used to run all files that match the pattern `*.tests.ts` and is defined in `package.json`.
		- {{video https://www.youtube.com/watch?v=poHAmXF-egw}}
		  collapsed:: true
			- {{youtube-timestamp 0}} let's now run these tests we open our terminal with control tilde and Visual Studio code and type NPM test this runs our testing framework and gives us our response we see our ad functions working fine but our divide function is failing on that last error check how can we fix this going into our units ts file finding our divide function we can add a simple check that says if B is zero we can throw a new error and I'm going to copy and paste what we expect / zero now if we run our tests once again using MPM test all of our checks passed we're now confident that our units are performing their functions properly
	-
	- ### Using Postman to Define Integration Tests
		- {{video https://www.youtube.com/watch?v=yWgBb_dpChs}}
		  collapsed:: true
			- {{youtube-timestamp 0}} since our cloud application has multiple moving parts it can also fail when those parts communicate with each other most of these connections provide responses that consume multiple functions and the way those functions interact might not always work as expected once again we can implement tests to help us understand whether we have this problem in this case these are called integration tests let's see how we can use integration tests within our code base once again we're going to navigate to our basic server and we're going to run NPM run dev to start our development server now our server has endpoints that have already been implemented and we expect them to work in a specific way we can now use postman using more powerful tools to test to make sure these endpoints work as expected let's see a simple example importing our collection for our basic server let's go to our root request here we're just getting the root endpoint
			- {{youtube-timestamp 65}} within our last tab we have tests now we can do some of those things we were looking for manually automatically we can write a test PM tests is post man's way of doing testing and declare what this test is testing for in this case status code is 200 we then look for the response to have a status of 200 we also want to make sure our body matches what we expect in our case welcome to the cloud clicking send we now get the body we expect however we can also explore the test results tab which give us two indicators that both of these tests pass if we go in and change this because let's say I'm bad at spelling click send we now have a failure for our body message indicating something is wrong with the interface between our client and our server let's go back and fix that now we can have more complicated tests but the real power comes in once we can begin testing all of these at if I click on this button and click run it will run our entire collection at
			- {{youtube-timestamp 141}} once once we're in our collection runner I can choose our environment and choose run to execute all of our tests now I'm confident all of our endpoints within our server adequately match our interface that we've declared and any system trying to interact with it will not fail
	-
- ## 5. Using Git Effectively for the Cloud #flashcard
  id:: 63de231a-d8f1-4dd7-bb81-640ce2a57576
	- {{video https://www.youtube.com/watch?v=iLGldKwcwqw}}
	  collapsed:: true
		- {{youtube-timestamp 0}} within this lesson we're going to be talking about how to use get effectively for cloud development it might be a little bit more complicated than you're used to but there's good reason for it if you use get before for personal or smaller projects you might be working on a single branch let's say master and committing small little changes to that branch as you're going you might start a project add some code add some code add something like an added login button and eventually you'll have a code base with all of your changes this might work out for you while you are doing small things but as you start to grow we'll run into problems let's explore why this might not work for you if we have more than one consumer of this codebase we'll start to have versioning problems developer a might be using version a this is our commit hash for example and developer B might be using an older or future commit version once we start deploying our code to the cloud we'll
		- {{youtube-timestamp 54}} have a server that's also consuming our code it's not clear whether we're using version a or version B or something completely different as we start to debug and add features this might become messy because things don't necessarily line up with what we expect and as we start to add more servers to our cloud this becomes extremely messy because each server might be in a slightly different state we can easily fix this the most simple change we can make is add an additional development branch this branch allows us to basically work the same way with our local process but now add major changes or major points where the code is stable we can commit those changes to our master branch now our master branch only contains big changes that we believe to be stable this solves our problem with our production code our server will probably be running off something like the master branch which we know to be the most recent codebase but this doesn't
		- {{youtube-timestamp 116}} necessarily solve our problem between developers a and B who might be editing off of the wrong version of development we can add some additional complexity to our process where we take our development branch and now we branch for individual features let's say we have the authentication feature someone's working on they can add small changes to that branch and then push that feature once it's ready to dev to test and make sure it works with all of the rest of our code another developer can pull feature to perform the same process and merge back into dev now we know that developer a is working on feature one and developer B is working off feature two and our server is on our master branch once we are confident that the code in our developer branch is stable we can promote that code to our master branch which is what our servers will ultimately be running and that's why our server running on the master branch will always have the most
		- {{youtube-timestamp 175}} up-to-date functional code we might run into some small issues here such as feature to being finished before feature 1 but the developer working on our first feature just needs to pull the latest version of dev before pushing their changes once all the code is integrated we can integrate that with master we can improve this process even further by adding a staging branch our cloud infrastructure might not run code the same exact way we're running locally for example I am usually developed locally on my macbook pro for web applications there are usually no server forms of MacBook Pros instead there are either container docker instances or other types of cloud infrastructure staged allows us to clone the type of infrastructure we're using on master to make sure all of the code we're working locally will also function while it's deployed now when we have changes that we believe to be stable in development we can promote to staging we can test on
		- {{youtube-timestamp 239}} the infrastructure will actually use without having our users potentially run into issues once we're confident in staging we can then promote to master where our production servers where our customers and users access our service can actually have the most up-to-date code now we have a very clear state of all of the players in our development process a and B are working on feature branches our test server is running our staging code and our production server is running our master code no one is confused on the state of software we can make this even better when we start to grow our team and have more complex systems we don't necessarily want everyone pushing code to those systems for example a malicious engineer could add something that emails all credit card numbers to their personal inbox we don't necessarily want this and having a protected branch structure gives us the ability to only allow trusted engineers to make these
		- {{youtube-timestamp 304}} kinds of sensitive changes there would be good practice to do independent or peer-reviewed code reviews between our development and stage environments any code in stage and any code and master is therefore trusted and we don't necessarily run into some of these security issues by having only trusted engineers have access to these code bases we have a second level of confidence that our code is functional and our code is secure we'll now be doing a quick exercise to bring our basic server repo up to these standards we'll be adding a development and a staging branch and add protection to our staging and master branch to prevent people from changing that code without authorization
	-
	- ![image.png](../assets/image_1675512973431_0.png)
	- ![image.png](../assets/image_1675512992750_0.png)
		- #### PR = Pull Request
			- In the video, the acronym "PR" stands for "pull request".
		- #### Fork the repository
			- Note, to follow along with Gabe’s walkthrough, please fork the [repository](https://github.com/udacity/cloud-developer/tree/master/course-02/exercises/udacity-c2-basic-server).
	- ### Configuring GitHub to Follow Our Process
		- {{video https://www.youtube.com/watch?v=fudKoIuOetc}}
		  collapsed:: true
			- {{youtube-timestamp 0}} Opening our basic server repo, we can see that we only have one branch, master. Let's add our development branch and our staging branch to bring ourselves up to our standard operating procedure. We'll create a branch called dev of master, then we'll create a branch called stage of dev. Fantastic. Now we have three branches. Let's navigate to settings, branches, and add a rule. We'll type in the master branch, and let's require pull requests reviews before merging. This will force code review before changes are merged into the master branch. Clicking "Create," it will require you to enter your password to make this sensitive change, and now we have our rule on our master branch. I'll let you do staging yourself as an exercise. Let's now see what it looks like when we tried to commit changes to master. First, let's open our master branch and let's just do something simple directly on this branch
			- {{youtube-timestamp 68}} by opening the Read Me and adding something that will change this file. Scrolling to the bottom, let's add this as a new branch as a patch one. This will create a new branch off of master and then create a pull request to merge those changes into the master branch. Proposing that change, we open the pull request. I will simply say update read me on our pull request, we'll create the request, and we'll see we need a review before we can merge this change. Now because I'm an administrator, I'm able to merge the pull requests, and since I have no one else working on this code base with me, I can't ask anyone to review it. So for now, I still have the ability to merge, but if I were a junior developer or someone new working on this code base who is working with me, I would want to review those changes before allowing that change to be made within our official pristine branches. There are additional reasons we want to have this kind of process change.
			- {{youtube-timestamp 134}} We've introduced the concepts of integration and unit tests. By having multiple stages running on the infrastructure, we will be using while having our production servers. This gives us an additional check to make sure that our systems will work when deployed. For example, if we have code that works in development, but we push it to staging and something fails, we will be alerted to this failure automatically. Once we fix that change, we're able to then promote to master. There are systems called continuous integration and continuous delivery, which will automatically watch for changes on our staging and master branches, and perform these actions for us. This is often referred to as DevOps or developer operations, and will dramatically speed up how quickly we can move from development to production code. One question that you might have lingering is, if something breaks catastrophically in the field,
			- {{youtube-timestamp 190}} this seems like it'll take a bit of time for everything to propagate through our system. You can do something called a hot fix or a patch, which does work right off of master. But, keep in mind only people who know what they're doing and you trust should have the ability to execute this kind of fix. Essentially, you work straight on master, commit that change as a pull request, which would then trigger your deployment automatically.
		-
- ## 6. Using Packages
	- As an engineer, you have to have the ability to asses the relevance and usefulness of libraries that make our tasks easier.
	- *Packages* are libraries of code that are written by other developers and made available through open-source licenses for the development community to use freely.
	- {{video https://www.youtube.com/watch?v=A03RTNt-7kY}}
	  collapsed:: true
		- {{youtube-timestamp 0}} we're leveraging packages to develop quickly without writing code for all of the complexities of web development when we're working in the server we want to make sure that we don't have to write code for every small piece of interaction let's take a closer look at our stack right now our server is running on our local computer our code is written using a framework called node and we're using Express as our web framework we didn't write any of those but because of the open source nature of programming today we can use software that other people wrote node comes along with a useful tool called node package manager that opens us up to a whole world of options for downloading and utilizing libraries that other people have wrote within our applications for example we don't want to process the incoming requests we're using Express and body parser for that remember each post request has a body payload which we can use to transmit important data to
		- {{youtube-timestamp 59}} the server body parser is one library to accomplish this task but is it something we should be using although there are libraries it's not necessarily the case that we should use it and a good engineer will follow a simple process to make sure that what we're using is both secure and usable by our team the first question to ask is is this even what we want will this help us accomplish the goal of using a library we can use the description to simply answer that question we then want to know if this is a popular framework or a popular library popular is not necessarily the only thing that matters but it generally is a good proxy for something that has good support and good maintenance and and use we want to know if this library will work we are using unit tests and integration tests within our code base but good libraries often have the same practice and really good libraries will show off that they are passing their builds and have 100% or high code
		- {{youtube-timestamp 122}} coverage so that we are confident that things will not break we also want to make sure that people are maintaining this codebase open issues are usually a good thing as long as they are being closed in a timely manner similarly we want something that's published within a reasonable time period I then want to know can I use it and reading through the documentation lightly just to make sure any questions I have are problems I have will have answers if I encounter those types of things in the future and finally can I use it freely licenses are extremely important to keep in mind when designing your stack and we want to make sure that the library's we're using are freely usable within our system so we don't run into any problems in the future mit Apache and GPL are some common ones that are pretty free but if there's anything that you don't know do a quick search to make sure that you can use it freely finally once you use it the node
		- {{youtube-timestamp 175}} package manager allows you to simply copy and paste the simple command line into your terminal to install that library MPM I body parser will add this automatically to our package manager and we can then use it through an import statement at the top of the file where we need that functionality as a quick exercise take a look at this package and come up with the determination of whether or not it's a good one to use and have a few reasons to justify that
	-
- ## 7.Lesson Recap
	- {{video https://www.youtube.com/watch?v=glqooounvrA&t=1s}}
	  collapsed:: true
		- {{youtube-timestamp 0}} in this lesson we've explored the basics of how to set up and best practices to follow when developing a production cloud environment these best practices will ensure that you and your team will write verifiable code which complies with most development requirements next we'll start provisioning resources in the cloud and apply what we've learned in practice
	-
	-