- #tags #Coursera #SW-Testing
- ## Week 1
	- ### Lesson 1: Introduction to Testing
		- **Verification** VS **Validation** #spaced
			- **Validation** is when we make sure that the actual product against the expected result from the user's perspective. It's a dynamic testing. It answers the question:
				- > Are we building the right product?
			- **Verification** is when we check that the software is free from bugs, technically speaking. Upon our internal requirements. It answers the question:
				- > Are we building the product right?
		- According to **Turing's halting problem**, it is theoretically not possible at all to check a perfect verification of one program by having another one checking it. #Curiosities
		- ![image.png](../assets/image_1659351422358_0.png)
	- ### Lesson 2: Why and How we Test
		- Types of tests: #spaced
			- **Unit tests:** testing individual classes / functions
			- **Integration tests:** testing packages / subsystems
			- **System tests:** testing the entire system
		- Even with **TDD**, there is *re-test* when you modify your code.
		- The function f(x) of a program is not continuous, so we can't derive its output (unlike other engineering fields) to test it. #Curiosities
		- *Optimistic* VS *Pessimistic* **testing** #flaschard
			- **Optimistic** means that the system gives you a lot of outputs. And you'll have to be careful because of the false positives.
				- > They say your program is right, when in fact it may be wrong in some cases.
			- **Pessimistic** means that the system gives you a lot of false negatives that you (the human) have to discard.
				- > They'll say your program is wrong when in fact your program may be right.
				-
		-
		- #### Questions
			- ##### Pregunta 3
			- Is testing a (primarily) optimistic or pessimistic verification technique?
			  
			  [ ] Optimistic
			  [ ] Pessimistic
			  #flaschard
				- [X] Optimistic
				  For the most part, testing is 'optimistic,' meaning that it will state that your program is correct when in fact it is incorrect for certain inputs.  Testing can be pessimistic, but only if the tests are incorrectly specified. If a tester incorrectly states the outcome of the test, then that test may incorrectly flag a program as misbehaving.
	- ### Lesson 3: What is a Test?
		- We have to provide inputs in order to test.
			- Valious picked up data i.e. border cases
		- How do we know if the output is right?
			- By the called *Oracle*
		- We have to automate testing output upon ***the oracle***.
		-
		- Anatomy of the parts of a Test: #flaschard
			- 1. **Setup**: How you put the software under test into the state under which the test input would make sense.
			  2. **Invocation**: Merely the execution of a single test case.
			  3. **Assessment**: The act of observing the behavior of that software under test.
			  4. **Teardown**: The opposite of the setup. Cleaning any resources.
- ---
- ## Week 2
	- ### Lesson 1: Testing Principles
		- **Dependability** is what you would expect.
			- We're interested in determining whether or not the software is dependable, that is whether it delivers a service such that we can rely on it.
		- **Service** is the system behavior as it's perceived by the user of the system.
		- A **failure** occurs when the delivered service deviates from the service specification.
		- An **error** is that part of the system that can lead to a failure.
			- Errors can be **latent** or **effective** *(active)*.
		- A **fault** is the root cause of the error.
			- It can be human caused (a *programmer's mistake*) or physical (a *short-circuit*)
		- So we have:
			- **Faults**, which are the cause of:
			- **Latent errors**, which become:
			- **Effective errors** if the system reaches a state where the error can manifest, which may cause:
			- **Failures** if the error causes a visible deviation from service from the user's perspective (*e.g. the program crashes*)
		- Achieving a dependable system involves utilizing four kinds of methods:
			- **Fault avoidance:** preventing by construction (*e.g. Java features*)
			- **Fault tolerance:** by redundancy.
			- **Error removal:** minimizing, by *verification*, the presence of latent errors. (This is where **testing** stands).
			- **Error forecasting:** estimating, by *evaluation*, the presence, the creation, and the consequences of errors.
		- Schema: ![image.png](../assets/image_1659425066865_0.png){:height 287, :width 472}
			- **Dependability**:
				- **Impairments** (*things we're trying to avoid*):
					- **Faults**: the mistakes and other things to avoid.
					- **Errors**: all programmers make mistakes.
					- **Failures**: eventually those mistakes introduce errors into the code. We want those not lead into failures.
				- **Means**:
					- **Validation**:
						- **Error removal**: we run tests against the software and we're going to remove some of those errors from the code.
						- **Error forecasting**: based on the above, we might do error forecasting that says: *"well, our big test suite is reliable"*
					- **Procurement**:
						- **Fault avoidance**: we can look at certain techniques that will prevent falls from being introduced in the first place
						- **Fault tolerance**: we know that there's a certain level of errors that we're going to find in code. And we're going to build things around those possibly erroneous components, in such a way that the system can continue to operate, even in the presence of errors.
				- **Measures**: two different *metrics*
					- **Reliability**: continuity of correct service
					- **Availability**: the readiness of the software to respond user requests.
					- **Safety**: absence of catastrophic consequences
					- **Integrity**: an
		-
		-
-
-
-
-
-
-